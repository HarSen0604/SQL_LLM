{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /opt/anaconda3/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: mysql-connector-python in /opt/anaconda3/lib/python3.11/site-packages (9.1.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: Flask in /opt/anaconda3/lib/python3.11/site-packages (2.2.5)\n",
      "Requirement already satisfied: Flask-Cors in /opt/anaconda3/lib/python3.11/site-packages (5.0.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/anaconda3/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from Flask) (2.2.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama mysql-connector-python transformers torch nltk Flask Flask-Cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import ollama\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aswinkumarv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords if you haven't done so\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to MySQL database and extract schema\n",
    "database = \"VenueScope\"\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Aswin@123\",\n",
    "    database=database\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE \n",
    "    TABLE_SCHEMA = '{database}'\n",
    "ORDER BY \n",
    "    TABLE_NAME, ORDINAL_POSITION;\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "schema_columns = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process schema to extract table and column names\n",
    "schema_info = {}\n",
    "for table_name, column_name in schema_columns:\n",
    "    if table_name not in schema_info:\n",
    "        schema_info[table_name] = []\n",
    "    schema_info[table_name].append(column_name)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Helper function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()  # Use mean pooling for sentence embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(query):\n",
    "    \"\"\"\n",
    "    Extract keywords from the query while removing stop words.\n",
    "    \"\"\"\n",
    "    words = query.lower().split()  # Simple tokenization\n",
    "    keywords = [word for word in words if word not in stop_words]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert table and column names to BERT embeddings\n",
    "def get_schema_embeddings(schema_info):\n",
    "    \"\"\"\n",
    "    Convert schema information (table and column names) into BERT embeddings.\n",
    "    \"\"\"\n",
    "    schema_embeddings = []\n",
    "    table_keys = []\n",
    "    \n",
    "    for table, columns in schema_info.items():\n",
    "        for column in columns:\n",
    "            text = f\"{table} {column}\"\n",
    "            embedding = get_bert_embeddings(text)\n",
    "            schema_embeddings.append(embedding)\n",
    "            table_keys.append(table)\n",
    "    \n",
    "    return schema_embeddings, table_keys\n",
    "\n",
    "schema_embeddings, table_keys = get_schema_embeddings(schema_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(user_query):\n",
    "    \"\"\"\n",
    "    Convert user query into BERT embeddings.\n",
    "    \"\"\"\n",
    "    return get_bert_embeddings(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys):\n",
    "    \"\"\"\n",
    "    Rank schema tables based on substring matching and BERT embeddings.\n",
    "    Priority is given to matches of user keywords, with embeddings as a secondary score.\n",
    "    \"\"\"\n",
    "    # Initialize scores\n",
    "    table_scores = {}\n",
    "\n",
    "    # Extract relevant keywords from the user query\n",
    "    relevant_keywords = set(keyword.lower() for keyword in user_keywords)\n",
    "\n",
    "    # Step 1: Apply string matching to prioritize relevant tables\n",
    "    for table in schema_info:\n",
    "        columns = schema_info[table]\n",
    "        table_lower = table.lower()\n",
    "\n",
    "        # Boost for matches of relevant keywords in table name\n",
    "        for keyword in relevant_keywords:\n",
    "            if keyword in table_lower:\n",
    "                table_scores[table] = table_scores.get(table, 0) + 2\n",
    "        \n",
    "        # Boost for matches of relevant keywords in column names\n",
    "        for column in columns:\n",
    "            for keyword in relevant_keywords:\n",
    "                if keyword in column.lower():\n",
    "                    table_scores[table] = table_scores.get(table, 0) + 2  # Adjust boost as needed\n",
    "\n",
    "    # Step 2: Apply embedding similarity as secondary ranking factor\n",
    "    for i, table in enumerate(table_keys):\n",
    "        similarity_score = cosine_similarity(user_query_embedding, schema_embeddings[i]).flatten()[0]\n",
    "        table_scores[table] = table_scores.get(table, 0) + similarity_score\n",
    "\n",
    "    # Step 3: Sort the tables based on the combined score (higher is better)\n",
    "    ranked_tables = sorted(table_scores.keys(), key=lambda x: table_scores[x], reverse=True)\n",
    "\n",
    "    # Step 4: Remove duplicates, maintaining order\n",
    "    unique_ranked_tables = []\n",
    "    seen_tables = set()\n",
    "    for table in ranked_tables:\n",
    "        if table not in seen_tables:\n",
    "            unique_ranked_tables.append(table)\n",
    "            seen_tables.add(table)\n",
    "\n",
    "    return unique_ranked_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Construct SQL query dynamically based on top-ranked table and columns\n",
    "def construct_sql_query(ranked_table_names, schema_info, user_query, top_n):\n",
    "    \"\"\"\n",
    "    Construct SQL query dynamically using Ollama based on top n-ranked tables and the user's query.\n",
    "    \"\"\"\n",
    "    # Get the top n-ranked tables\n",
    "    top_ranked_tables = ranked_table_names[:top_n]\n",
    "    \n",
    "    # Collect schema information for the top-ranked tables\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_ranked_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Pass the schema info and user query to Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='duckdb-nsql',\n",
    "        messages=[{'role': 'user', 'content': f\"This is the schema: \\n{schema_info_str}\\n{user_query}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# top_n = 2  # Set how many top-ranked tables to include\n",
    "# Get the response from Ollama based on the user's query and ranked schema\n",
    "# ollama_query = construct_sql_query(ranked_table_names, schema_info, user_query, top_n)\n",
    "# print(\"Ollama SQL Query:\", ollama_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Modify ranking to consider foreign keys and relationships\n",
    "def get_foreign_key_relations(cursor, schema_info, database):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships from the INFORMATION_SCHEMA for the given database.\n",
    "    Returns a dictionary mapping tables to their related tables via foreign keys.\n",
    "    \"\"\"\n",
    "    foreign_key_query = f\"\"\"\n",
    "    SELECT \n",
    "        TABLE_NAME, \n",
    "        COLUMN_NAME, \n",
    "        REFERENCED_TABLE_NAME, \n",
    "        REFERENCED_COLUMN_NAME\n",
    "    FROM \n",
    "        INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "    WHERE \n",
    "        TABLE_SCHEMA = '{database}' \n",
    "        AND REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(foreign_key_query)\n",
    "    foreign_keys = cursor.fetchall()\n",
    "    \n",
    "    fk_relations = {}\n",
    "    for table, column, ref_table, ref_column in foreign_keys:\n",
    "        if table not in fk_relations:\n",
    "            fk_relations[table] = []\n",
    "        fk_relations[table].append((column, ref_table, ref_column))\n",
    "    \n",
    "    return fk_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings):\n",
    "    \"\"\"\n",
    "    Compare user query embedding with column embeddings and rank columns based on relevance.\n",
    "    \"\"\"\n",
    "    column_scores = []\n",
    "    user_query_embedding = user_query_embedding.reshape(1, -1)  # Reshape user query embedding to 2D\n",
    "\n",
    "    for column, embedding in zip(column_names, column_embeddings):\n",
    "        embedding = embedding.reshape(1, -1)  # Reshape column embedding to 2D\n",
    "        # Compute similarity between the user query and each column embedding (cosine similarity)\n",
    "        similarity_score = cosine_similarity(user_query_embedding, embedding)[0][0]  # Extract scalar\n",
    "        column_scores.append((column, similarity_score))\n",
    "\n",
    "    # Sort columns by relevance (higher similarity score first)\n",
    "    column_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return column_scores\n",
    "\n",
    "\n",
    "\n",
    "def construct_and_execute_query(cursor, ranked_table_names, schema_info, user_query, top_n, max_attempts=5):\n",
    "    \"\"\"\n",
    "    retry_construct_and_execute_query_with_column_reranking\n",
    "    For each top_n ranked table, rank its columns by relevance to the user query,\n",
    "    re-rank tables based on the relevance of columns, and generate SQL query if relevant.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    ollama_query = \"\"\n",
    "    user_query_embedding = get_bert_embeddings(user_query)  # Embed the user's query\n",
    "\n",
    "    while not success and attempt < max_attempts:\n",
    "        try:\n",
    "            # Increment attempt count\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "\n",
    "            # Iterate over top-ranked tables to find the most relevant column match\n",
    "            for table_name in ranked_table_names[:top_n]:\n",
    "                column_names = schema_info[table_name]  # Get columns for the table\n",
    "                column_embeddings = get_bert_embeddings(column_names)  # Embed the column names\n",
    "\n",
    "                # Rank columns based on their relevance to the user's query\n",
    "                ranked_columns = rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings)\n",
    "                print(f\"Ranked columns for table {table_name}: {ranked_columns}\")\n",
    "\n",
    "                # Check if the top-ranked column has sufficient relevance\n",
    "                top_column, relevance_score = ranked_columns[0]\n",
    "                print(f\"Top column: {top_column}, Relevance score: {relevance_score}\")\n",
    "\n",
    "                if relevance_score > 0.5:  # Threshold for relevance (can be adjusted)\n",
    "                    print(f\"Proceeding with table {table_name} and top column {top_column}\")\n",
    "\n",
    "                    # Generate SQL query using Ollama with the relevant table and columns\n",
    "                    ollama_query = construct_sql_query([table_name], schema_info, user_query, top_n=1)\n",
    "                    print(\"Generated Query from Ollama:\", ollama_query)\n",
    "\n",
    "                    # Try executing the query\n",
    "                    cursor.execute(ollama_query)\n",
    "                    results = cursor.fetchall()\n",
    "                    success = True  # Mark success if query executes successfully\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Relevance score too low for table {table_name}. Trying the next table...\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Re-ranking columns and trying the next table...\")\n",
    "\n",
    "    # If successful, return the results\n",
    "    if success:\n",
    "        print(\"Query executed successfully!\")\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Failed after {max_attempts} attempts.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_keys(cursor, schema_info):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships for the tables in schema_info.\n",
    "    \"\"\"\n",
    "    foreign_keys = {}\n",
    "    for table in schema_info.keys():\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                REFERENCED_TABLE_NAME, \n",
    "                REFERENCED_COLUMN_NAME\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "            WHERE \n",
    "                TABLE_NAME = '{table}' AND \n",
    "                TABLE_SCHEMA = 'your_database_name' AND \n",
    "                REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "        \"\"\")\n",
    "        foreign_keys[table] = cursor.fetchall()\n",
    "    return foreign_keys\n",
    "\n",
    "\n",
    "def construct_sql_query_for_ollama(top_tables, schema_info, user_query):\n",
    "    \"\"\"\n",
    "    Construct SQL query using schema information for the top tables.\n",
    "    \"\"\"\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Prepare the final query for Ollama\n",
    "    query_for_ollama = f\"{schema_info_str}\\n{user_query}\"\n",
    "    return query_for_ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "Total lines read: 1\n"
     ]
    }
   ],
   "source": [
    "def getQueriesFromFile(file_path):\n",
    "    \"\"\"\n",
    "    Read the queries from a file and generate BERT embeddings for each.\n",
    "    \"\"\"\n",
    "    print(\"Reading file...\")\n",
    "    queries = []\n",
    "    \n",
    "    # Open the file and read queries\n",
    "    with open(file_path, 'r') as file:\n",
    "        queries = file.readlines()\n",
    "    \n",
    "    print(f\"Total lines read: {len(queries)}\")\n",
    "    \n",
    "    # Generate embeddings for each query\n",
    "    for i in range (len(queries)):\n",
    "        queries[i] = queries[i].strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Assuming the queries are in 'queries.txt' file\n",
    "file_path = 'queries.txt'\n",
    "queries = getQueriesFromFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['booked_venue', 'club_head_details', 'club_list', 'club_head', 'venue_list']\n",
      "Attempt 1 to generate and execute the query...\n",
      "Get the names of the clubs starting with the letter 'A'\n",
      "Query Generated:  SELECT club_name FROM club_list WHERE club_name LIKE 'A%';\n",
      "Output: [('AeroModeling Club',), ('Animal Welfare Club',), ('Anti Drug Club',), ('Artificial Intelligence & Robotics',), ('Association of Serious Quizzers',), ('Astronomy Club',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of attempts for executing the query\n",
    "max_attempts = 5\n",
    "top_n = 2  # Adjust the number of top tables as needed\n",
    "\n",
    "for query in queries:\n",
    "    print(top_n)\n",
    "    user_keywords = extract_keywords(query)\n",
    "    user_query_embedding = get_bert_embeddings(query)\n",
    "\n",
    "    # Rank schema tables based on query relevance and uniqueness\n",
    "    ranked_table_names = rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys)\n",
    "    print(ranked_table_names)\n",
    "\n",
    "    top_tables_after_re_ranking = ranked_table_names[:top_n]  # Get the top tables after re-ranking\n",
    "\n",
    "    # Retry logic for generating and executing the query\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    while not success and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "            top_n += 1\n",
    "            top_tables_after_re_ranking = ranked_table_names[:top_n]  # Get the top tables after re-ranking\n",
    "\n",
    "            # Construct the query for Ollama\n",
    "            ollama_query = construct_sql_query_for_ollama(top_tables_after_re_ranking, schema_info, query)\n",
    "\n",
    "            # Pass this to Ollama for query generation\n",
    "            stream = ollama.chat(\n",
    "                model='duckdb-nsql',\n",
    "                messages=[{'role': 'user', 'content': ollama_query}],\n",
    "                stream=True,\n",
    "            )\n",
    "\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk['message']['content']\n",
    "\n",
    "            # Execute the generated SQL query\n",
    "            cursor.execute(response)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            # If successful, print results and mark success\n",
    "            success = True\n",
    "            print(query)\n",
    "            \n",
    "            if success:\n",
    "                print(\"Query Generated: \" + response + \"\\nOutput: \" + str(results) + \"\\n\")\n",
    "                top_n = 2\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Retrying...\")  # Log the retry\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            print(\"Retrying...\")  # Log the retry\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to execute query after {max_attempts} attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Youth Red Cross Society',)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cursor and database connection\n",
    "cursor.close()\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
