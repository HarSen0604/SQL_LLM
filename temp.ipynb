{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in ./venv/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: mysql-connector-python in ./venv/lib/python3.12/site-packages (9.0.0)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./venv/lib/python3.12/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.12/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama mysql-connector-python transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hareeshsenthil/PW1/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import ollama\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to MySQL database and extract schema\n",
    "database = \"VenueScope\"\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Karaikudi-630002\",\n",
    "    database=database\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE \n",
    "    TABLE_SCHEMA = '{database}'\n",
    "ORDER BY \n",
    "    TABLE_NAME, ORDINAL_POSITION;\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "schema_columns = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hareeshsenthil/PW1/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process schema to extract table and column names\n",
    "schema_info = {}\n",
    "for table_name, column_name in schema_columns:\n",
    "    if table_name not in schema_info:\n",
    "        schema_info[table_name] = []\n",
    "    schema_info[table_name].append(column_name)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Helper function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()  # Use mean pooling for sentence embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(user_query):\n",
    "    \"\"\"\n",
    "    Simple keyword extraction to match schema terms.\n",
    "    \"\"\"\n",
    "    keywords = user_query.lower().split()  # Split the query into words\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert table and column names to BERT embeddings\n",
    "def get_schema_embeddings(schema_info):\n",
    "    \"\"\"\n",
    "    Convert schema information (table and column names) into BERT embeddings.\n",
    "    \"\"\"\n",
    "    schema_embeddings = []\n",
    "    table_keys = []\n",
    "    \n",
    "    for table, columns in schema_info.items():\n",
    "        for column in columns:\n",
    "            text = f\"{table} {column}\"\n",
    "            embedding = get_bert_embeddings(text)\n",
    "            schema_embeddings.append(embedding)\n",
    "            table_keys.append(table)\n",
    "    \n",
    "    return schema_embeddings, table_keys\n",
    "\n",
    "schema_embeddings, table_keys = get_schema_embeddings(schema_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(user_query):\n",
    "    \"\"\"\n",
    "    Convert user query into BERT embeddings.\n",
    "    \"\"\"\n",
    "    return get_bert_embeddings(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys):\n",
    "    \"\"\"\n",
    "    Generalized function to rank schema tables based on keyword matches and BERT embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize table scores\n",
    "    table_scores = {}\n",
    "\n",
    "    # Step 1: Apply string matching across table and column names\n",
    "    for table, columns in schema_info.items():\n",
    "        table_lower = table.lower()\n",
    "        table_scores[table] = 0  # Initialize score for the table\n",
    "        \n",
    "        # Boost score if user keywords match in the table name or its columns\n",
    "        for keyword in user_keywords:\n",
    "            # Boost for keyword in table name\n",
    "            if keyword in table_lower:\n",
    "                table_scores[table] += 2\n",
    "\n",
    "            # Boost for keyword in column names\n",
    "            for column in columns:\n",
    "                column_lower = column.lower()\n",
    "                if keyword in column_lower:\n",
    "                    table_scores[table] += 1  # Smaller boost for column matches\n",
    "\n",
    "    # Step 2: Apply embedding similarity as a secondary ranking factor\n",
    "    for i, table in enumerate(table_keys):\n",
    "        similarity_score = cosine_similarity(user_query_embedding, schema_embeddings[i]).flatten()[0]\n",
    "        table_scores[table] = table_scores.get(table, 0) + similarity_score\n",
    "\n",
    "    # Step 3: Sort the tables based on the combined score (higher is better)\n",
    "    ranked_tables = sorted(table_scores.keys(), key=lambda x: table_scores[x], reverse=True)\n",
    "\n",
    "    # Step 4: Remove duplicates, maintaining order\n",
    "    unique_ranked_tables = []\n",
    "    seen_tables = set()\n",
    "    for table in ranked_tables:\n",
    "        if table not in seen_tables:\n",
    "            unique_ranked_tables.append(table)\n",
    "            seen_tables.add(table)\n",
    "\n",
    "    return unique_ranked_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Construct SQL query dynamically based on top-ranked table and columns\n",
    "def construct_sql_query(ranked_table_names, schema_info, user_query, top_n):\n",
    "    \"\"\"\n",
    "    Construct SQL query dynamically using Ollama based on top n-ranked tables and the user's query.\n",
    "    \"\"\"\n",
    "    # Get the top n-ranked tables\n",
    "    top_ranked_tables = ranked_table_names[:top_n]\n",
    "    \n",
    "    # Collect schema information for the top-ranked tables\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_ranked_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Pass the schema info and user query to Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='duckdb-nsql',\n",
    "        messages=[{'role': 'user', 'content': f\"This is the schema: \\n{schema_info_str}\\n{user_query}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "\n",
    "    return response\n",
    "\n",
    "# top_n = 2  # Set how many top-ranked tables to include\n",
    "# Get the response from Ollama based on the user's query and ranked schema\n",
    "# ollama_query = construct_sql_query(ranked_table_names, schema_info, user_query, top_n)\n",
    "# print(\"Ollama SQL Query:\", ollama_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Modify ranking to consider foreign keys and relationships\n",
    "def get_foreign_key_relations(cursor, schema_info, database):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships from the INFORMATION_SCHEMA for the given database.\n",
    "    Returns a dictionary mapping tables to their related tables via foreign keys.\n",
    "    \"\"\"\n",
    "    foreign_key_query = f\"\"\"\n",
    "    SELECT \n",
    "        TABLE_NAME, \n",
    "        COLUMN_NAME, \n",
    "        REFERENCED_TABLE_NAME, \n",
    "        REFERENCED_COLUMN_NAME\n",
    "    FROM \n",
    "        INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "    WHERE \n",
    "        TABLE_SCHEMA = '{database}' \n",
    "        AND REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(foreign_key_query)\n",
    "    foreign_keys = cursor.fetchall()\n",
    "    \n",
    "    fk_relations = {}\n",
    "    for table, column, ref_table, ref_column in foreign_keys:\n",
    "        if table not in fk_relations:\n",
    "            fk_relations[table] = []\n",
    "        fk_relations[table].append((column, ref_table, ref_column))\n",
    "    \n",
    "    return fk_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings):\n",
    "    \"\"\"\n",
    "    Compare user query embedding with column embeddings and rank columns based on relevance.\n",
    "    \"\"\"\n",
    "    column_scores = []\n",
    "    user_query_embedding = user_query_embedding.reshape(1, -1)  # Reshape user query embedding to 2D\n",
    "\n",
    "    for column, embedding in zip(column_names, column_embeddings):\n",
    "        embedding = embedding.reshape(1, -1)  # Reshape column embedding to 2D\n",
    "        # Compute similarity between the user query and each column embedding (cosine similarity)\n",
    "        similarity_score = cosine_similarity(user_query_embedding, embedding)[0][0]  # Extract scalar\n",
    "        column_scores.append((column, similarity_score))\n",
    "\n",
    "    # Sort columns by relevance (higher similarity score first)\n",
    "    column_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return column_scores\n",
    "\n",
    "\n",
    "\n",
    "def construct_and_execute_query(cursor, ranked_table_names, schema_info, user_query, top_n, max_attempts=5):\n",
    "    \"\"\"\n",
    "    retry_construct_and_execute_query_with_column_reranking\n",
    "    For each top_n ranked table, rank its columns by relevance to the user query,\n",
    "    re-rank tables based on the relevance of columns, and generate SQL query if relevant.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    ollama_query = \"\"\n",
    "    user_query_embedding = get_bert_embeddings(user_query)  # Embed the user's query\n",
    "\n",
    "    while not success and attempt < max_attempts:\n",
    "        try:\n",
    "            # Increment attempt count\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "\n",
    "            # Iterate over top-ranked tables to find the most relevant column match\n",
    "            for table_name in ranked_table_names[:top_n]:\n",
    "                column_names = schema_info[table_name]  # Get columns for the table\n",
    "                column_embeddings = get_bert_embeddings(column_names)  # Embed the column names\n",
    "\n",
    "                # Rank columns based on their relevance to the user's query\n",
    "                ranked_columns = rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings)\n",
    "                print(f\"Ranked columns for table {table_name}: {ranked_columns}\")\n",
    "\n",
    "                # Check if the top-ranked column has sufficient relevance\n",
    "                top_column, relevance_score = ranked_columns[0]\n",
    "                print(f\"Top column: {top_column}, Relevance score: {relevance_score}\")\n",
    "\n",
    "                if relevance_score > 0.5:  # Threshold for relevance (can be adjusted)\n",
    "                    print(f\"Proceeding with table {table_name} and top column {top_column}\")\n",
    "\n",
    "                    # Generate SQL query using Ollama with the relevant table and columns\n",
    "                    ollama_query = construct_sql_query([table_name], schema_info, user_query, top_n=1)\n",
    "                    print(\"Generated Query from Ollama:\", ollama_query)\n",
    "\n",
    "                    # Try executing the query\n",
    "                    cursor.execute(ollama_query)\n",
    "                    results = cursor.fetchall()\n",
    "                    success = True  # Mark success if query executes successfully\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Relevance score too low for table {table_name}. Trying the next table...\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Re-ranking columns and trying the next table...\")\n",
    "\n",
    "    # If successful, return the results\n",
    "    if success:\n",
    "        print(\"Query executed successfully!\")\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Failed after {max_attempts} attempts.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_keys(cursor, schema_info):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships for the tables in schema_info.\n",
    "    \"\"\"\n",
    "    foreign_keys = {}\n",
    "    for table in schema_info.keys():\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                REFERENCED_TABLE_NAME, \n",
    "                REFERENCED_COLUMN_NAME\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "            WHERE \n",
    "                TABLE_NAME = '{table}' AND \n",
    "                TABLE_SCHEMA = 'your_database_name' AND \n",
    "                REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "        \"\"\")\n",
    "        foreign_keys[table] = cursor.fetchall()\n",
    "    return foreign_keys\n",
    "\n",
    "def construct_sql_query_for_ollama(top_tables, schema_info, user_query):\n",
    "    \"\"\"\n",
    "    Construct SQL query using schema information for the top tables.\n",
    "    \"\"\"\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # You can also include foreign key information if necessary\n",
    "    foreign_keys = get_foreign_keys(cursor, schema_info)\n",
    "    for table, keys in foreign_keys.items():\n",
    "        if keys:\n",
    "            schema_info_str += f\"Foreign keys for {table}:\\n\"\n",
    "            for key in keys:\n",
    "                schema_info_str += f\"  - {key[0]} -> {key[1]}({key[2]})\\n\"\n",
    "\n",
    "    # Prepare the final query for Ollama\n",
    "    query_for_ollama = f\"{schema_info_str}\\n{user_query}\"\n",
    "    return query_for_ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "Total lines read: 3\n"
     ]
    }
   ],
   "source": [
    "def getQueriesFromFile(file_path):\n",
    "    \"\"\"\n",
    "    Read the queries from a file and generate BERT embeddings for each.\n",
    "    \"\"\"\n",
    "    print(\"Reading file...\")\n",
    "    queries = []\n",
    "    \n",
    "    # Open the file and read queries\n",
    "    with open(file_path, 'r') as file:\n",
    "        queries = file.readlines()\n",
    "    \n",
    "    print(f\"Total lines read: {len(queries)}\")\n",
    "    \n",
    "    # Generate embeddings for each query\n",
    "    for i in range (len(queries)):\n",
    "        queries[i] = queries[i].strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Assuming the queries are in 'queries.txt' file\n",
    "file_path = 'queries.txt'\n",
    "queries = getQueriesFromFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['booked_venue', 'club_list', 'venue_list', 'club_head_details', 'club_head']\n",
      "List out the names of the heads from all the clubs\n",
      " SELECT club_name FROM club_list [('AeroModeling Club',), ('Animal Welfare Club',), ('Anti Drug Club',), ('Artificial Intelligence & Robotics',), ('Association of Serious Quizzers',), ('Astronomy Club',), ('Book Readers Club',), ('CAP Nature Club',), ('Cyber Security Club',), ('Dramatix Club',), ('English Literary Society',), ('Entrepreneurs Club',), ('Fine Arts Club',), ('Finverse Club',), ('Global Leaders Forum',), ('Higher Education Forum',), ('Industry Interaction Forum',), ('Martial Arts Club',), ('PSG Tech Chronicle Club',), ('Paathshala Club',), ('Radio Hub',), ('Rotaract Club',), ('SPIC-MACAY Heritage Club',), ('Student Research Council',), ('Tech Music',), ('Women Development Cell',), ('Youth Outreach Club',), ('Youth Red Cross Society',), ('Yuva Tourism Club',)]\n",
      "['club_head_details', 'booked_venue', 'club_list', 'club_head', 'venue_list']\n",
      "Get the names of the clubs starting with the letter 'A'\n",
      " SELECT club_name FROM club_list WHERE club_name LIKE 'A%'; [('AeroModeling Club',), ('Animal Welfare Club',), ('Anti Drug Club',), ('Artificial Intelligence & Robotics',), ('Association of Serious Quizzers',), ('Astronomy Club',)]\n",
      "['club_list', 'club_head_details', 'club_head', 'venue_list', 'booked_venue']\n",
      "What is the name of the club where the person with head ID 1 is the head?\n",
      " SELECT club_name FROM club_list WHERE club_id = (SELECT club_id FROM club_head WHERE head_id = 1); [('SPIC-MACAY Heritage Club',)]\n"
     ]
    }
   ],
   "source": [
    "# After re-ranking the top tables\n",
    "top_n = 3  # Adjust the number of top tables as needed\n",
    "\n",
    "for query in queries:\n",
    "    user_keywords = extract_keywords(query)\n",
    "    # Step 6: Get query embedding\n",
    "    user_query_embedding = get_bert_embeddings(query)\n",
    "\n",
    "    # Rank schema tables based on query relevance and uniqueness\n",
    "    ranked_table_names = rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys)\n",
    "    print(ranked_table_names)\n",
    "\n",
    "    top_tables_after_re_ranking = ranked_table_names[:top_n]  # Replace with your actual top tables\n",
    "\n",
    "    # Construct the query for Ollama\n",
    "    ollama_query = construct_sql_query_for_ollama(top_tables_after_re_ranking, schema_info, query)\n",
    "\n",
    "    # Pass this to Ollama for query generation\n",
    "    stream = ollama.chat(\n",
    "        model='duckdb-nsql',\n",
    "        messages=[{'role': 'user', 'content': ollama_query}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "\n",
    "    # Execute the generated SQL query\n",
    "    cursor.execute(response)\n",
    "    results = cursor.fetchall()\n",
    "    print(query)\n",
    "    print(response, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPIC-MACAY Heritage Club',)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cursor and database connection\n",
    "cursor.close()\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
