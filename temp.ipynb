{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in ./venv/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: mysql-connector-python in ./venv/lib/python3.12/site-packages (9.0.0)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./venv/lib/python3.12/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.12/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ollama mysql-connector-python transformers torch nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import ollama\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hareeshsenthil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords if you haven't done so\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to MySQL database and extract schema\n",
    "database = \"VenueScope\"\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Karaikudi-630002\",\n",
    "    database=database\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    TABLE_NAME, \n",
    "    COLUMN_NAME \n",
    "FROM \n",
    "    INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE \n",
    "    TABLE_SCHEMA = '{database}'\n",
    "ORDER BY \n",
    "    TABLE_NAME, ORDINAL_POSITION;\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "schema_columns = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hareeshsenthil/PW1/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process schema to extract table and column names\n",
    "schema_info = {}\n",
    "for table_name, column_name in schema_columns:\n",
    "    if table_name not in schema_info:\n",
    "        schema_info[table_name] = []\n",
    "    schema_info[table_name].append(column_name)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Helper function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()  # Use mean pooling for sentence embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(query):\n",
    "    \"\"\"\n",
    "    Extract keywords from the query while removing stop words.\n",
    "    \"\"\"\n",
    "    words = query.lower().split()  # Simple tokenization\n",
    "    keywords = [word for word in words if word not in stop_words]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert table and column names to BERT embeddings\n",
    "def get_schema_embeddings(schema_info):\n",
    "    \"\"\"\n",
    "    Convert schema information (table and column names) into BERT embeddings.\n",
    "    \"\"\"\n",
    "    schema_embeddings = []\n",
    "    table_keys = []\n",
    "    \n",
    "    for table, columns in schema_info.items():\n",
    "        for column in columns:\n",
    "            text = f\"{table} {column}\"\n",
    "            embedding = get_bert_embeddings(text)\n",
    "            schema_embeddings.append(embedding)\n",
    "            table_keys.append(table)\n",
    "    \n",
    "    return schema_embeddings, table_keys\n",
    "\n",
    "schema_embeddings, table_keys = get_schema_embeddings(schema_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(user_query):\n",
    "    \"\"\"\n",
    "    Convert user query into BERT embeddings.\n",
    "    \"\"\"\n",
    "    return get_bert_embeddings(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys):\n",
    "    \"\"\"\n",
    "    Rank schema tables based on substring matching and BERT embeddings.\n",
    "    Priority is given to matches of user keywords, with embeddings as a secondary score.\n",
    "    \"\"\"\n",
    "    # Initialize scores\n",
    "    table_scores = {}\n",
    "\n",
    "    # Extract relevant keywords from the user query\n",
    "    relevant_keywords = set(keyword.lower() for keyword in user_keywords)\n",
    "\n",
    "    # Step 1: Apply string matching to prioritize relevant tables\n",
    "    for table in schema_info:\n",
    "        columns = schema_info[table]\n",
    "        table_lower = table.lower()\n",
    "\n",
    "        # Boost for matches of relevant keywords in table name\n",
    "        for keyword in relevant_keywords:\n",
    "            if keyword in table_lower:\n",
    "                table_scores[table] = table_scores.get(table, 0) + 2\n",
    "        \n",
    "        # Boost for matches of relevant keywords in column names\n",
    "        for column in columns:\n",
    "            for keyword in relevant_keywords:\n",
    "                if keyword in column.lower():\n",
    "                    table_scores[table] = table_scores.get(table, 0) + 2  # Adjust boost as needed\n",
    "\n",
    "    # Step 2: Apply embedding similarity as secondary ranking factor\n",
    "    for i, table in enumerate(table_keys):\n",
    "        similarity_score = cosine_similarity(user_query_embedding, schema_embeddings[i]).flatten()[0]\n",
    "        table_scores[table] = table_scores.get(table, 0) + similarity_score\n",
    "\n",
    "    # Step 3: Sort the tables based on the combined score (higher is better)\n",
    "    ranked_tables = sorted(table_scores.keys(), key=lambda x: table_scores[x], reverse=True)\n",
    "\n",
    "    # Step 4: Remove duplicates, maintaining order\n",
    "    unique_ranked_tables = []\n",
    "    seen_tables = set()\n",
    "    for table in ranked_tables:\n",
    "        if table not in seen_tables:\n",
    "            unique_ranked_tables.append(table)\n",
    "            seen_tables.add(table)\n",
    "\n",
    "    return unique_ranked_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Construct SQL query dynamically based on top-ranked table and columns\n",
    "def construct_sql_query(ranked_table_names, schema_info, user_query, top_n):\n",
    "    \"\"\"\n",
    "    Construct SQL query dynamically using Ollama based on top n-ranked tables and the user's query.\n",
    "    \"\"\"\n",
    "    # Get the top n-ranked tables\n",
    "    top_ranked_tables = ranked_table_names[:top_n]\n",
    "    \n",
    "    # Collect schema information for the top-ranked tables\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_ranked_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Pass the schema info and user query to Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='duckdb-nsql',\n",
    "        messages=[{'role': 'user', 'content': f\"This is the schema: \\n{schema_info_str}\\n{user_query}\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# top_n = 2  # Set how many top-ranked tables to include\n",
    "# Get the response from Ollama based on the user's query and ranked schema\n",
    "# ollama_query = construct_sql_query(ranked_table_names, schema_info, user_query, top_n)\n",
    "# print(\"Ollama SQL Query:\", ollama_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Modify ranking to consider foreign keys and relationships\n",
    "def get_foreign_key_relations(cursor, schema_info, database):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships from the INFORMATION_SCHEMA for the given database.\n",
    "    Returns a dictionary mapping tables to their related tables via foreign keys.\n",
    "    \"\"\"\n",
    "    foreign_key_query = f\"\"\"\n",
    "    SELECT \n",
    "        TABLE_NAME, \n",
    "        COLUMN_NAME, \n",
    "        REFERENCED_TABLE_NAME, \n",
    "        REFERENCED_COLUMN_NAME\n",
    "    FROM \n",
    "        INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "    WHERE \n",
    "        TABLE_SCHEMA = '{database}' \n",
    "        AND REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(foreign_key_query)\n",
    "    foreign_keys = cursor.fetchall()\n",
    "    \n",
    "    fk_relations = {}\n",
    "    for table, column, ref_table, ref_column in foreign_keys:\n",
    "        if table not in fk_relations:\n",
    "            fk_relations[table] = []\n",
    "        fk_relations[table].append((column, ref_table, ref_column))\n",
    "    \n",
    "    return fk_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings):\n",
    "    \"\"\"\n",
    "    Compare user query embedding with column embeddings and rank columns based on relevance.\n",
    "    \"\"\"\n",
    "    column_scores = []\n",
    "    user_query_embedding = user_query_embedding.reshape(1, -1)  # Reshape user query embedding to 2D\n",
    "\n",
    "    for column, embedding in zip(column_names, column_embeddings):\n",
    "        embedding = embedding.reshape(1, -1)  # Reshape column embedding to 2D\n",
    "        # Compute similarity between the user query and each column embedding (cosine similarity)\n",
    "        similarity_score = cosine_similarity(user_query_embedding, embedding)[0][0]  # Extract scalar\n",
    "        column_scores.append((column, similarity_score))\n",
    "\n",
    "    # Sort columns by relevance (higher similarity score first)\n",
    "    column_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return column_scores\n",
    "\n",
    "\n",
    "\n",
    "def construct_and_execute_query(cursor, ranked_table_names, schema_info, user_query, top_n, max_attempts=5):\n",
    "    \"\"\"\n",
    "    retry_construct_and_execute_query_with_column_reranking\n",
    "    For each top_n ranked table, rank its columns by relevance to the user query,\n",
    "    re-rank tables based on the relevance of columns, and generate SQL query if relevant.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    ollama_query = \"\"\n",
    "    user_query_embedding = get_bert_embeddings(user_query)  # Embed the user's query\n",
    "\n",
    "    while not success and attempt < max_attempts:\n",
    "        try:\n",
    "            # Increment attempt count\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "\n",
    "            # Iterate over top-ranked tables to find the most relevant column match\n",
    "            for table_name in ranked_table_names[:top_n]:\n",
    "                column_names = schema_info[table_name]  # Get columns for the table\n",
    "                column_embeddings = get_bert_embeddings(column_names)  # Embed the column names\n",
    "\n",
    "                # Rank columns based on their relevance to the user's query\n",
    "                ranked_columns = rank_columns_by_relevance(user_query_embedding, column_names, column_embeddings)\n",
    "                print(f\"Ranked columns for table {table_name}: {ranked_columns}\")\n",
    "\n",
    "                # Check if the top-ranked column has sufficient relevance\n",
    "                top_column, relevance_score = ranked_columns[0]\n",
    "                print(f\"Top column: {top_column}, Relevance score: {relevance_score}\")\n",
    "\n",
    "                if relevance_score > 0.5:  # Threshold for relevance (can be adjusted)\n",
    "                    print(f\"Proceeding with table {table_name} and top column {top_column}\")\n",
    "\n",
    "                    # Generate SQL query using Ollama with the relevant table and columns\n",
    "                    ollama_query = construct_sql_query([table_name], schema_info, user_query, top_n=1)\n",
    "                    print(\"Generated Query from Ollama:\", ollama_query)\n",
    "\n",
    "                    # Try executing the query\n",
    "                    cursor.execute(ollama_query)\n",
    "                    results = cursor.fetchall()\n",
    "                    success = True  # Mark success if query executes successfully\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Relevance score too low for table {table_name}. Trying the next table...\")\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Re-ranking columns and trying the next table...\")\n",
    "\n",
    "    # If successful, return the results\n",
    "    if success:\n",
    "        print(\"Query executed successfully!\")\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Failed after {max_attempts} attempts.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_keys(cursor, schema_info):\n",
    "    \"\"\"\n",
    "    Extract foreign key relationships for the tables in schema_info.\n",
    "    \"\"\"\n",
    "    foreign_keys = {}\n",
    "    for table in schema_info.keys():\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT \n",
    "                COLUMN_NAME, \n",
    "                REFERENCED_TABLE_NAME, \n",
    "                REFERENCED_COLUMN_NAME\n",
    "            FROM \n",
    "                INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "            WHERE \n",
    "                TABLE_NAME = '{table}' AND \n",
    "                TABLE_SCHEMA = 'your_database_name' AND \n",
    "                REFERENCED_TABLE_NAME IS NOT NULL;\n",
    "        \"\"\")\n",
    "        foreign_keys[table] = cursor.fetchall()\n",
    "    return foreign_keys\n",
    "\n",
    "\n",
    "def construct_sql_query_for_ollama(top_tables, schema_info, user_query):\n",
    "    \"\"\"\n",
    "    Construct SQL query using schema information for the top tables.\n",
    "    \"\"\"\n",
    "    schema_info_str = \"\"\n",
    "    for table in top_tables:\n",
    "        columns = schema_info[table]\n",
    "        schema_info_str += f\"Table {table}: Columns ({', '.join(columns)})\\n\"\n",
    "\n",
    "    # Prepare the final query for Ollama\n",
    "    query_for_ollama = f\"{schema_info_str}\\n{user_query}\"\n",
    "    return query_for_ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file...\n",
      "Total lines read: 3\n"
     ]
    }
   ],
   "source": [
    "def getQueriesFromFile(file_path):\n",
    "    \"\"\"\n",
    "    Read the queries from a file and generate BERT embeddings for each.\n",
    "    \"\"\"\n",
    "    print(\"Reading file...\")\n",
    "    queries = []\n",
    "    \n",
    "    # Open the file and read queries\n",
    "    with open(file_path, 'r') as file:\n",
    "        queries = file.readlines()\n",
    "    \n",
    "    print(f\"Total lines read: {len(queries)}\")\n",
    "    \n",
    "    # Generate embeddings for each query\n",
    "    for i in range (len(queries)):\n",
    "        queries[i] = queries[i].strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Assuming the queries are in 'queries.txt' file\n",
    "file_path = 'queries.txt'\n",
    "queries = getQueriesFromFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['club_list', 'club_head_details', 'club_head', 'booked_venue', 'venue_list']\n",
      "Attempt 1 to generate and execute the query...\n",
      "List out the details of the club heads from all the clubs\n",
      " SELECT * FROM club_head_details\n",
      "JOIN club_head ON club_head_details.head_id = club_head.head_id; [(1, 'John Doe', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'john_doe@random.com', 28, 1), (2, 'Jane Smith', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'jane_smith@random.com', 27, 2), (3, 'Sam Johnson', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'sam_johnson@random.com', 5, 3), (4, 'Emily Davis', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'emily_davis@random.com', 10, 4), (5, 'Chris Brown', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'chris_brown@random.com', 21, 5), (6, 'Anna Lee', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'anna_lee@random.com', 22, 6), (7, 'Michael White', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'michael_white@random.com', 14, 7), (8, 'Emma Wilson', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'emma_wilson@random.com', 8, 8), (9, 'David Harris', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'david_harris@random.com', 18, 9), (10, 'Sophia Thompson', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'sophia_thompson@random.com', 7, 10), (11, 'James Martin', '9876543210', '$2b$12$ilIxE0u0kOW5zrIPwxOr1OJkhq2LOb/VIpEls08OwL.n7KbmvO03a', 'james_martin@random.com', 11, 11), (12, 'Olivia Taylor', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'olivia_taylor@random.com', 12, 12), (13, 'Benjamin Walker', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'benjamin_walker@random.com', 20, 13), (14, 'Ava Scott', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'ava_scott@random.com', 23, 14), (15, 'Liam Adams', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'liam_adams@random.com', 26, 15), (16, 'Isabella Nelson', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'isabella_nelson@random.com', 25, 16), (17, 'Noah Young', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'noah_young@random.com', 13, 17), (18, 'Mia Allen', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'mia_allen@random.com', 16, 18), (19, 'Lucas Hall', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'lucas_hall@random.com', 1, 19), (20, 'Charlotte King', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'charlotte_king@random.com', 3, 20), (21, 'Mason Wright', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'mason_wright@random.com', 29, 21), (22, 'Amelia Moore', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'amelia_moore@random.com', 2, 22), (23, 'Ethan Green', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'ethan_green@random.com', 6, 23), (24, 'Harper Lewis', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'harper_lewis@random.com', 15, 24), (25, 'Alexander Clark', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'alexander_clark@random.com', 17, 25), (26, 'Abigail Turner', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'abigail_turner@random.com', 24, 26), (27, 'Elijah Baker', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'elijah_baker@random.com', 4, 27), (28, 'Evelyn Phillips', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'evelyn_phillips@random.com', 19, 28), (29, 'Logan Mitchell', '9876543210', '$2b$12$vOZNRAHGOe8ZccVo5rBCAeMLNH/zwYLtsrXfgBgvrRByAPkcQ5Wy.', 'logan_mitchell@random.com', 9, 29)]\n",
      "\n",
      "2\n",
      "['booked_venue', 'club_head_details', 'club_list', 'club_head', 'venue_list']\n",
      "Attempt 1 to generate and execute the query...\n",
      "Get the names of the clubs starting with the letter 'A'\n",
      " SELECT club_name FROM club_list WHERE club_name LIKE 'A%'; [('AeroModeling Club',), ('Animal Welfare Club',), ('Anti Drug Club',), ('Artificial Intelligence & Robotics',), ('Association of Serious Quizzers',), ('Astronomy Club',)]\n",
      "\n",
      "2\n",
      "['club_head_details', 'club_head', 'club_list', 'booked_venue', 'venue_list']\n",
      "Attempt 1 to generate and execute the query...\n",
      "What is the name of the club where the person with head ID 1 is the head?\n",
      " SELECT club_name FROM club_list WHERE club_id = (SELECT club_id FROM club_head WHERE head_id = 1); [('Youth Red Cross Society',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of attempts for executing the query\n",
    "max_attempts = 5\n",
    "top_n = 2  # Adjust the number of top tables as needed\n",
    "\n",
    "for query in queries:\n",
    "    print(top_n)\n",
    "    user_keywords = extract_keywords(query)\n",
    "    user_query_embedding = get_bert_embeddings(query)\n",
    "\n",
    "    # Rank schema tables based on query relevance and uniqueness\n",
    "    ranked_table_names = rank_schemas_v2(user_keywords, schema_info, user_query_embedding, schema_embeddings, table_keys)\n",
    "    print(ranked_table_names)\n",
    "\n",
    "    top_tables_after_re_ranking = ranked_table_names[:top_n]  # Get the top tables after re-ranking\n",
    "\n",
    "    # Retry logic for generating and executing the query\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    while not success and attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            print(f\"Attempt {attempt} to generate and execute the query...\")\n",
    "            top_n += 1\n",
    "            top_tables_after_re_ranking = ranked_table_names[:top_n]  # Get the top tables after re-ranking\n",
    "\n",
    "            # Construct the query for Ollama\n",
    "            ollama_query = construct_sql_query_for_ollama(top_tables_after_re_ranking, schema_info, query)\n",
    "\n",
    "            # Pass this to Ollama for query generation\n",
    "            stream = ollama.chat(\n",
    "                model='duckdb-nsql',\n",
    "                messages=[{'role': 'user', 'content': ollama_query}],\n",
    "                stream=True,\n",
    "            )\n",
    "\n",
    "            response = \"\"\n",
    "            for chunk in stream:\n",
    "                response += chunk['message']['content']\n",
    "\n",
    "            # Execute the generated SQL query\n",
    "            cursor.execute(response)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            # If successful, print results and mark success\n",
    "            success = True\n",
    "            print(query)\n",
    "            print(response, results)\n",
    "            print()\n",
    "            if success:\n",
    "                top_n = 2\n",
    "\n",
    "        except mysql.connector.Error as err:\n",
    "            print(f\"Query execution failed with error: {err}\")\n",
    "            print(\"Retrying...\")  # Log the retry\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            print(\"Retrying...\")  # Log the retry\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to execute query after {max_attempts} attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anna Lee',),\n",
       " ('Ava Scott',),\n",
       " ('Amelia Moore',),\n",
       " ('Alexander Clark',),\n",
       " ('Abigail Turner',)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cursor and database connection\n",
    "cursor.close()\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
